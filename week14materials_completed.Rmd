---
title: "Week 14 materials"
author: "Biagio Palese"
date: "`r format(Sys.time(), '%d %B, %Y')`" 
output: 
  html_document:
    theme: flatly
    toc: TRUE
    toc_float:
      collapsed: false
      smooth_scroll: true
    code_download: TRUE
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE, cache = F}
knitr::opts_chunk$set(
  echo = TRUE,
  error = TRUE,
  warning= FALSE,
  message= FALSE)
```

# Optimizing Average Daily Rate: A Predictive Analytics Case Study in the Hotel Sector



### Objectives

-   Use a case study to review the data modeling process as a whole.


#### Installing and Loading necessary packages

```{r}
library(tidymodels)
library(corrplot)
library(tidyverse)
hotel_data <- read.csv("case_study_hotel_data.csv")

```

## Case Study Description

OptimaHotel Insights, a leading consultancy company specializing in revenue management, provides predictive analytics services for a diverse range of hotels across different chains. The company is known for its advanced analytical capabilities that not only offer insights into current pricing strategies but also predict future pricing to maximize revenue and occupancy rates.

**Objective**

OptimaHotel Insights has been tasked by multiple hotel chains to develop a predictive model that can accurately forecast daily rates based on various hotel features and market dynamics. The goal is to enable these hotels to proactively adjust their pricing strategies, enhancing profitability and maintaining competitive advantages in fluctuating markets.

**Scope of Analysis**

The `hotel_data` dataset compiled for this initiative includes data from over 3000 hotels, encompassing features such as star ratings, location quality, customer satisfaction indices, amenities, occupancy rates, and historical average daily rate. This broad range of data points ensures that the predictive model can account for a variety of factors influencing prices.

**Analytical Approach**
OptimaHotel Insights employs a robust suite of regression techniques tailored to tackle the complex dynamics of hotel pricing. This approach includes using regression methods that enhance the model's ability to manage interdependencies among variables and improve predictive accuracy (e.g.,Lasso regression, which excels in feature selection and helps reduce model complexity by eliminating insignificant predictors).

These techniques are collectively utilized to develop a predictive model that is capable of accurately forecasting hotel prices based on a variety of factors, ensuring the model remains both flexible and robust against overfitting. The methodology is chosen to optimally balance detail with simplicity, providing actionable insights that can be readily applied across different hotel properties and market conditions.

**Deliverables**

OptimaHotel Insights will provide each hotel chain with:

- A fully functional predictive pricing model with high prediction accuracy.

- A comprehensive report detailing model insights, feature importance, and predicted impacts on ADR.

- Offer strategic recommendations on how to leverage the predictive analysis into their daily operations to adjust prices dynamically.

**Expected Outcomes**

By leveraging the predictive pricing model, the hotels are expected to:

- Identifying relevant variables, highlighting current issues, and provide insights on how to adjust average daily room prices accordingly.


**Monitoring and Adjustment**

Post-deployment, OptimaHotel Insights will continue to monitor the performance of the pricing model, providing regular updates and adjustments to ensure its continued relevance and accuracy as market conditions change. 


### Step 1: Exploring the dataset

The `hotel_data` dataset used in this case study contains comprehensive information on various attributes and performance metrics of 3000 hotels across major U.S. cities. It is meticulously curated (e.g., normalization transformations not needed) to support predictive analyses focused on predicting and optimizing the average daily rate (ADR) for these properties. This dataset includes realistic and detailed data, encompassing a wide range of features crucial for developing predictive pricing models. Attributes such as the number of rooms, location quality, available amenities, customer satisfaction indices, and others are included to provide a holistic view of factors that influence hotel pricing strategies.

```{r}
glimpse(hotel_data)
#access the data dictionary by clicking on data_dictionary_hotel_dataset.html file

hotel_data |> 
  summarise(min_average_daily_rate	= min(average_daily_rate), avg_average_daily_rate	= mean(average_daily_rate	), median_average_daily_rate	= median(average_daily_rate	), max_average_daily_rate	= max(average_daily_rate	), sd_average_daily_rate	=sd(average_daily_rate	))

#we can see a normal distribution
hotel_data |>
  ggplot(aes(x=average_daily_rate	)) + 
  geom_histogram()

#outliers with boxplot
hotel_data |> 
  ggplot(aes(x=average_daily_rate	)) + 
  geom_boxplot()

# Does city have an impact? no variation due to city 
hotel_data |>
  group_by(city) |> 
  summarise(min_average_daily_rate	= min(average_daily_rate	), avg_average_daily_rate	= mean(average_daily_rate	), median_average_daily_rate	= median(average_daily_rate	), max_average_daily_rate	= max(average_daily_rate	), sd_average_daily_rate	=sd(average_daily_rate	), count =n) |> view()

hotel_data |>
  group_by(hotel_chain) |> 
  summarise(min_average_daily_rate	= min(average_daily_rate	), avg_average_daily_rate	= mean(average_daily_rate	), median_average_daily_rate	= median(average_daily_rate	), max_average_daily_rate	= max(average_daily_rate	), sd_average_daily_rate	=sd(average_daily_rate	))


hotel_data |> 
  summarise(min_revPAR	= min(revPAR), avg_revPAR	= mean(revPAR	), median_revPAR	= median(revPAR	), max_revPAR	= max(revPAR	), sd_revPAR	=sd(revPAR	))

hotel_data |>
  ggplot(aes(x=revPAR	)) + 
  geom_histogram()

hotel_data |> 
  ggplot(aes(x=revPAR	)) + 
  geom_boxplot()

hotel_data |>
  group_by(city) |> 
  summarise(min_revPAR	= min(revPAR	), avg_revPAR	= mean(revPAR	), median_revPAR	= median(revPAR	), max_revPAR	= max(revPAR	), sd_revPAR	=sd(revPAR	))

hotel_data |>
  group_by(hotel_chain) |> 
  summarise(min_revPAR	= min(revPAR	), avg_revPAR	= mean(revPAR	), median_revPAR	= median(revPAR	), max_revPAR	= max(revPAR	), sd_revPAR	=sd(revPAR	))

```


### Step 2: Cleaning and manipulating the data

Based on the above exploration the following minor manipulations (dataset was on purpose almost perfect.. almost never the case in real life) are required:
```{r}
hotel_clean <- hotel_data |> 
    filter(average_daily_rate	>60 &average_daily_rate	<130) |> #removes outliers and gives normal distribution
   filter(revPAR	>5 &revPAR	<118) |> 
   mutate(miles_distance_to_city_center= distance_to_city_center/1.6)
```


#### Activity 1: Dataset exploration. Write additional code to explore more your dataset. [write code in the chunk below; for help or queries, use Teams RStudio - Forum channel] - Homework

```{r}

```


### Step 3: Checking correlations
Given that we are working on a new dataset and that we are not super familiar with hotel data, it is worth to take a look at the correlation of our numerical variables. I will check them in different matrices to increase readability and keep ADR as it is our dependent variable (we will not visualize them because long variables name need arguments adjustments or to be renamed to improve chart but it is not the purpose of this class). 
```{r}
correlation_matrix1 <- hotel_clean |> 
  select_if(is.numeric) |> 
  select(2:6,10) |> 
  cor(use = "complete.obs")
correlation_matrix1

correlation_matrix2 <- hotel_clean |> 
  select_if(is.numeric) |> 
  select(9:14) |> 
  cor(use = "complete.obs")
correlation_matrix2

correlation_matrix3 <- hotel_clean |> 
  select_if(is.numeric) |> 
  select(10, 15:19) |> 
  cor(use = "complete.obs")
correlation_matrix3
```


### Step 4: Data Splitting

When it comes to data modeling (and model evaluation), one of the most adopted method is to split the data into a training set and a test set from the beginning. Here’s how this simple method can be implemented:

```{r}

# Split the data
set.seed(123)# setting a seed will ensure reproducibility
hotel_split <- initial_split(hotel_clean, prop = 0.75) # define the split and its data proportion
hotel_train <- training(hotel_split) #create a train set
hotel_test <- testing(hotel_split)#create a test set
```


## Making prediction and evaluating different models' prediction performance using data splitting

In this case study, we explore the predictions' outcome between using `lm` for standard linear regression and `glmnet` for regularized linear regression (Lasso). We will do that using three different recipes. Recipe1 will be based on a multiple linear regression model with just revPAR and competitors_average_price (two predictors with the highest correlation with average_daily_rate), recipe2 will add location_quality, star_rating and season; finally recipe3 will use all the independent variables in our dataset.

#### Step 5: Setting up the recipes

```{r}
# Recipe 1
recipe1 <- recipe(average_daily_rate ~ revPAR + competitors_average_price, data = hotel_train)#not making any changes so I actually can specify the formula in the workflow but let's keep it as it is for semplicity
recipe1
# Recipe 2
recipe2 <- recipe(average_daily_rate ~ revPAR + competitors_average_price+ location_quality+ star_rating + season, data = hotel_train)|> 
  step_dummy(all_nominal_predictors()) 
recipe2

# Recipe 3
recipe3 <- recipe(average_daily_rate ~ ., data = hotel_train)|> 
  step_dummy(all_nominal_predictors()) 
recipe3

```

#### Activity 2: Creating more recipes. Write the code to create a recipe4 and recipe5 in which you add one more categorical variable and one more interval variable to those included in recipe2. Make sure to use different variables for recipe4 and recipe5 so that you can compare the results. [write code in the chunk below; for help or queries, use Teams RStudio - Forum channel] - Homework

### Step 6: Specifying the models

We specify the models using `parsnip`: one with the `lm` engine for linear regression and another with the `glmnet` engine for Lasso regression, which includes a penalty term to regularize the coefficients.

```{r}
# Linear regression with lm
linear_mod_reg <- linear_reg() |>
  set_engine("lm") |>
  set_mode("regression")

# Lasso regularized linear regression with glmnet
linear_mod_lasso <- linear_reg(penalty = 0.1, mixture = 1) |>
  set_engine("glmnet") |>
  set_mode("regression")

```

#### Activity 3: Defining another model. Write the code to define a linear_mod_ridge model for ridge regression [write code in the chunk below; for help or queries, use Teams RStudio - Forum channel] - Homework

```{r}

```


### Step 7: Fitting models using the workflow

Next, we fit the models to the `hotel_train` dataset, so that we can compare their predictions performance on `hotel_test`. This is accomplished by embedding the model specifications within `workflows` that also incorporate our preprocessing `recipes`.

```{r}
# Workflows for linear regression
recipe1_workflow_reg <- workflow() |>
  add_recipe(recipe1) |>
  add_model(linear_mod_reg) |>
  fit(data = hotel_train)#we create and fit the workflow on just the train set
recipe1_workflow_reg |> print() |> tidy()#check model results

recipe2_workflow_reg <- workflow() |>
  add_recipe(recipe2) |>
  add_model(linear_mod_reg) |>
  fit(data = hotel_train)#we create and fit the workflow on just the train set
recipe2_workflow_reg |> print() |> tidy()#check model results


recipe3_workflow_reg <- workflow() |>
  add_recipe(recipe3) |>
  add_model(linear_mod_reg) |>
  fit(data = hotel_train)#we create and fit the workflow on just the train set
recipe3_workflow_reg |> print() |> tidy()#check model results
# Workflows for lasso
recipe1_workflow_lasso <- workflow() |>
  add_recipe(recipe1) |>
  add_model(linear_mod_lasso) |>
  fit(data = hotel_train)#we create and fit the workflow on just the train set
recipe1_workflow_lasso |> print() |> tidy()#check model results

recipe2_workflow_lasso <- workflow() |>
  add_recipe(recipe2) |>
  add_model(linear_mod_lasso) |>
  fit(data = hotel_train)#we create and fit the workflow on just the train set
recipe2_workflow_lasso |> print() |> tidy()#check model results

recipe3_workflow_lasso <- workflow() |>
  add_recipe(recipe3) |>
  add_model(linear_mod_lasso) |>
  fit(data = hotel_train)#we create and fit the workflow on just the train set
recipe3_workflow_lasso |> print() |> tidy()#check model results
```

#### Activity 4: Create more workflows. Write the code to create the workflows that include linear_mod_ridge model and recipe1, recipe2, recipe3, recipe4 and recipe5. Remember to also create workflows for linear_mod_reg and linear_mod_lasso for recipe4 and recipe5. So, you should create 9 additional models. [write code in the chunk below; for help or queries, use Teams RStudio - Forum channel] - Homework

```{r}

```


## Model Comparison and Evaluation

With the models fitted, so far we just took a quick look at the results. However, before learning how to interpret all the models results, we will learn to assess their performance (there is no point in interpreting them if they perform poorly or are "bad" models). This involves making predictions on our test set, and evaluating the models using metrics suited for regression tasks, such as RMSE (Root Mean Squared Error) or MAE (Mean Absolute Error) or R² (Coefficient of Determination).

### Step 8: Making predictions on the test set

```{r}
# Making predictions for the linear regression models workflows
recipe1_predictions_reg <- predict(recipe1_workflow_reg, new_data = hotel_test) |> # making the prediction on test data (unseen data)
  bind_cols(hotel_test)# add original columns

recipe1_predictions_reg |> 
  select(average_daily_rate, .pred) |> view()#check the prediction on test data against actual value (truth)

recipe2_predictions_reg <- predict(recipe2_workflow_reg, new_data = hotel_test) |> 
  bind_cols(hotel_test)# add original columns

recipe2_predictions_reg |> 
  select(average_daily_rate, .pred) |> view()#check the prediction on test data against actual value (truth)

recipe3_predictions_reg <- predict(recipe3_workflow_reg, new_data = hotel_test) |> 
  bind_cols(hotel_test)# add original columns

recipe3_predictions_reg |> 
  select(average_daily_rate, .pred) |> view()#check the prediction on test data against actual value (truth)
    
# Making predictions for the lasso models workflows
recipe1_predictions_lasso <- predict(recipe1_workflow_lasso, new_data = hotel_test) |> 
  bind_cols(hotel_test)# add original columns


recipe1_predictions_lasso |> 
  select(average_daily_rate, .pred) |> view()#check the prediction on test data against actual value (truth)

recipe2_predictions_lasso <- predict(recipe2_workflow_lasso, new_data = hotel_test) |> 
  bind_cols(hotel_test)# add original columns


recipe2_predictions_lasso |> 
  select(average_daily_rate, .pred) |> view()#check the prediction on test data against actual value (truth)


recipe3_predictions_lasso <- predict(recipe3_workflow_lasso, new_data = hotel_test) |> 
  bind_cols(hotel_test)# add original columns


recipe3_predictions_lasso |> 
  select(average_daily_rate, .pred) |> view()#check the prediction on test data against actual value (truth)
```

If you guys noticed when we run predictions using the linear regression model with recipe3 (recipe3_predictions_reg) we got the same warning again. The warning indicated some rank-deficiency fit. This is usually due to:

-   the presence of highly correlated predictors (multicollinearity). Multicollinearity indicates that we have redundant information from some highly correlated predictors, making it difficult to distinguish their individual effects on the dependent variable.

-   the presence of too many predictors in our model. If the model includes too many predictor variables relative to the number of observations, it can lead to a situation where the predictors cannot be uniquely identified. Meaning that there isn't enough independent information in the data to estimate the model's parameters (the coefficients of the predictor variables) with precision.

Possible solutions are check for multicollinearity and using correlation matrix to identify and then remove highly correlated predictors. Or reduce the number of predictors by performing Principal Components Analysis (PCA). PCA is beyond the scope of this class but regularization methods (e.g., Ridge or Lasso regression) are designed to handle multicollinearity (Ridge in particular), high number of predictors (Lasso in particular) and overfitting. For this reason we don't get a warning when we run the lasso model using recipe3.

In conclusion, while the linear regression model with recipe3 runs (and produce just a warning), we should not attempt to interpret the results because they can misleading and lead to bad decisions. For illustrative scope we will keep that model in but we know that in real life we will have to make changes to what predictors are included in it.



### Step 9: Creating Model Metrics to Assess Model Prediction Performance

While seeing the predictions next to the actual house values can already provide some insights on the goodness of the model. In regression analysis, model performance is evaluated using specific metrics that quantify the model's accuracy and ability to generalize. Three fundamental metrics are Root Mean Squared Error (RMSE), Mean Absolute Error (MAE), and R-squared (R²):

-   **Root Mean Squared Error (RMSE):**
    -   **What It Measures:** RMSE calculates the square root of the average squared differences between the predicted and actual values. It represents the standard deviation of the residuals (prediction errors).
    -   **Interpretation:** A lower RMSE value indicates better model performance, with 0 being the ideal score. It quantifies how much, on average, the model's predictions deviate from the actual values.
    -   **Something to consider:** RMSE is sensitive to outliers. High RMSE values may suggest the presence of large errors in some predictions, highlighting potential model weaknesses.
-   **Mean Absolute Error (MAE)**:
    -   **What It Measures**: MAE quantifies the average magnitude of the errors between the predicted values and the actual values, focusing solely on the size of errors without considering their direction. It reflects the average distance between predicted and actual values across all predictions.
    -   **Interpretation**: MAE values range from 0 to infinity, with lower values indicating better model performance. A MAE of 0 means the model perfectly predicts the target variable, although such a scenario is extremely rare in practice.
    -   **Something to consider**: MAE provides a straightforward and easily interpretable measure of model prediction accuracy. It's particularly useful because it's robust to outliers, making it a reliable metric when dealing with real-world data that may contain anomalies. MAE helps in understanding the typical error magnitude the model might have in its predictions, offering clear insights into the model’s performance.
-   **R-squared (R²):**
    -   **What It Measures:** R², also known as the coefficient of determination, quantifies the proportion of the variance in the dependent variable that is predictable from the independent variables. It provides a measure of how well observed outcomes are replicated by the model.
    -   **Interpretation:** R² values range from 0 to 1, where higher values indicate better model fit. An R² of 1 suggests the model perfectly predicts the target variable.
    -   **Something to consider:** R² offers an insight into the goodness of fit of the model. However, it does not indicate if the model is the appropriate one for your data, nor does it reflect on the accuracy of the predictions.

```{r}
model_reg1 <- recipe1_predictions_reg |>   metrics(truth = average_daily_rate, estimate = .pred) |>
  mutate(model="reg_model_recipe1")## Bringing them all together. Remember: rmse the smaller the better; mae the smaller the better; rsq the bigger the better
model_reg1

model_reg2 <- recipe2_predictions_reg |>   metrics(truth = average_daily_rate, estimate = .pred) |>
  mutate(model="reg_model_recipe2")
model_reg2

model_reg3 <- recipe3_predictions_reg |>   metrics(truth = average_daily_rate, estimate = .pred) |>
  mutate(model="reg_model_recipe3")## Bringing them all together. Remember: rmse the smaller the better; mae the smaller the better; rsq the bigger the better
model_reg3

model_lasso1 <- recipe1_predictions_lasso |> 
  metrics(truth = average_daily_rate, estimate = .pred) |> 
  mutate(model="lasso_model_recipe1")
model_lasso1  

model_lasso2 <- recipe2_predictions_lasso |> 
  metrics(truth = average_daily_rate, estimate = .pred) |> 
  mutate(model="lasso_model_recipe2")
model_lasso2 

model_lasso3 <- recipe3_predictions_lasso |> 
  metrics(truth = average_daily_rate, estimate = .pred) |> 
  mutate(model="lasso_model_recipe3")
model_lasso3 

all_models_metrics <- bind_rows(model_reg1, model_reg2, model_reg3, model_lasso1,model_lasso2,model_lasso3) #bringing all model together

all_models_metrics |> arrange(.metric, .estimate)#sort them to identify the best models --> rmse the smaller the better; mae the smaller the better; rsq the bigger the better. 
```

#### Identify the best model using models' metrics

Decide which model to proceed with should be based on these metrics, considering RMSE [lower values better], MAE [lower values better] and R² [higher values better]. Sometimes the best model is the one that gives the best compromise among those metrics. They are not always in agreement. Moreover, keep in mind that the choice of model might also depend on other factors such as:

-   Interpretability and complexity of the model.

-   Computational resources and time available.

-   The specific requirements of your application or project.

The lasso regression models with recipe3 seems to be the best model when it comes to these metrics. It has the lowest MAE, lowest RMSE and highest R-Squared. Lasso recipe3 is clearly the best model in this case. However, that model might be harder to interpret than reg_model_recipe2. So,we will take in consideration also reg_model_recipe2 even though it explain "only" 50.7% (second to lowest value) of the variance of average_daily_rate. The next step will be to visualize both models above to better understand which one is the "optimal" model in this case.


### Step 10: Visualizing Prediction Results

Visualizing the prediction results can provide additional insights into model performance. You might plot the predicted vs. actual values or the residuals to visually assess how well the model is capturing the underlying relationship in the data.

```{r}
# Creating the scatter plot for the best model in terms of performance metrics
ggplot(recipe3_predictions_lasso, aes(x = average_daily_rate, y = .pred)) +
  geom_point(alpha = 0.4, color= "gold") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "gold4") +
  labs(x = "Actual Values", y = "Predicted Values", title = "Best Model Predicted vs. Actual Values")+
  scale_x_continuous(labels = label_comma()) +  # Adjust X-axis labels
  scale_y_continuous(labels = label_comma())    # Adjust Y-axis labels


```

When interpreting scatter plots comparing actual values to predicted values, especially for regression models, the ideal scenario is for the points to closely align with the diagonal line. The diagonal line represents perfect prediction accuracy, where the predicted values exactly match the actual values. 

The chart above clearly shows how this model does a great job in predicting our dependent variable. However, there is some room for improvement because it tends to overpredict small values and under predict large values. While the points are scattered around the diagonal, the distance from them and the line should be minimized especially for smaller and larger values.

In general, here's how to interpret the charts and understand the nuances of good versus concerning patterns:

#### Points Around the Diagonal

-   **Interpretation:** If your linear model (lm) points are scattered around the diagonal, it suggests that the model's predictions are generally in agreement with the actual values. The closer the points are to the diagonal, the more accurate the predictions.

-   **Good Chart Indicators:**

  -   **Tight Cluster Around Diagonal:** Points tightly clustered around the diagonal line indicate high prediction accuracy.

  -   **Even Spread Without Bias:** Points evenly spread above and below the diagonal line suggest that the model doesn't systematically overestimate or underestimate the target variable.

#### Actions Based on Chart Interpretation

-   **For lm with Good Alignment:** If the lm model shows points closely around the diagonal, it suggests your model is performing reasonably well. Consider if any slight biases or patterns arise. In our case the model seems to overall perform very well. However, it could do better for small and large values of average_daily_rate. 

#### What about the same chart but with the other model?
```{r}
ggplot(recipe2_predictions_reg, aes(x = average_daily_rate, y = .pred)) +
  geom_point(alpha = 0.4, color="darkblue") +
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "blue") +
  labs(x = "Actual Values", y = "Predicted Values", title = "Other Model Predicted vs. Actual Values")+
  scale_x_continuous(labels = label_comma()) +  # Adjust X-axis labels
  scale_y_continuous(labels = label_comma())    # Adjust Y-axis labels
```

We can clearly see that the other model also has the same tendency on overpredicting small values and underpredicting large values. However, it is also less precise in making the prediction with points further from the diagonal compared to the best model.

#### Visualizing the comparison in one chart

When we look at the above charts, it seems clear that one of the models outperform the other. However, in situation like this it's easier to make the comparisons among them when the models are in the same plot. More specifically, two plots are beneficial to make the comparison:

1.  **Overlaid Residuals Plot**: The overlaid residuals plot displays the residuals (the differences between the actual and predicted values) of the models across the range of predicted values. Here’s what to look for:

[Zero Line]{.underline}: The dashed line at y = 0 represents perfect predictions where the predicted values match the actual values exactly. *Residuals above this line indicate underpredictions, and residuals below indicate overpredictions*.

[Spread of Residuals]{.underline}: A tightly clustered spread of points around the zero line suggests a model with smaller errors. Wider spreads indicate larger errors. Patterns: Ideally, residuals should be randomly distributed around the zero line, with no discernible pattern. Patterns or trends might indicate systematic errors in a model.

```{r}
ggplot() +
  geom_point(data = recipe3_predictions_lasso, aes(x = .pred, y = average_daily_rate - .pred, color = "Best Model")) +
  geom_point(data = recipe2_predictions_reg, aes(x = .pred, y = average_daily_rate - .pred, color = "Other Model"), alpha=0.4) +
    geom_hline(yintercept = 0, linetype = "dashed") +
    scale_color_manual(values = c("Best Model" = "gold", "Other Model" = "blue")) +
    labs(x = "Predicted", y = "Residuals", title = "Overlayed Residuals Plot") +
    theme_minimal() +
    guides(color = guide_legend(title = "Model"))+
  scale_x_continuous(labels = label_comma()) +  # Adjust X-axis labels
  scale_y_continuous(labels = label_comma())    # Adjust Y-axis labels

```

2.  **Overlaid Prediction Error Plot**: This plot shows the predicted values against the actual values for all models:

[Diagonal Line]{.underline}: Represents the line of perfect prediction. The closer the points to this line, the more accurate the predictions. 

[Dispersion]{.underline}: Points closely clustered around the diagonal line indicate high accuracy. If points for one model are closer to the line than another, that model has generally performed better. Bias: If points systematically deviate to one side of the line, it might indicate bias in the model predictions.


```{r}
ggplot() +
    geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "black") +
    geom_point(data = recipe3_predictions_lasso, aes(x = average_daily_rate, y = .pred, color = "Best Model")) +
  geom_point(data = recipe2_predictions_reg, aes(x = average_daily_rate, y = .pred, color = "Other Model"), alpha=0.4) +
    scale_color_manual(values = c("Best Model" = "gold", "Other Model" = "darkblue" )) +
    labs(x = "Actual", y = "Predicted", title = "Overlayed Prediction Error Plot") +
    theme_minimal() +
    guides(color = guide_legend(title = "Model"), shape = guide_legend(title = "Model"))+
  scale_x_continuous(labels = label_comma()) +  # Adjust X-axis labels
  scale_y_continuous(labels = label_comma())    # Adjust Y-axis labels

```

Visual interpretations provide valuable insights into model performance but should be complemented with quantitative metrics (like RMSE, R²) for a comprehensive evaluation. Scatter plots help identify patterns or biases not immediately apparent from numerical metrics alone, offering a more intuitive understanding of model behavior

By carefully analyzing these metrics and visualizations, you can make an informed decision about which model best meets your needs, taking into account both quantitative performance and practical considerations.

## What Makes a Good Model (Optimal)?
Now that we learned also how to visualize the models results it is time to summarize what makes a good model. Here are three elements you should always keep in mind before proceeding to model selection and interpretation:

-   **Accuracy:**
    -   **Essential for Trustworthy Predictions:** Accurate models closely align predictions with actual observed values, enhancing the reliability of the insights or decisions derived from the model.
    -   **Balancing Precision and Usability:** While striving for accuracy, it's crucial to ensure the model remains applicable and interpretable within its intended context.
-   **Generalizability:**
    -   **Performance Across Diverse Data:** A model's ability to perform well on unseen data, not just the data it was trained on, is vital for its utility in real-world applications.
    -   **Robustness to Overfitting:** Generalizable models resist overfitting, where a model learns the noise in the training data to the detriment of its performance on new data.
-   **Simplicity:**
    -   **The Principle of Occam's Razor:** When two models offer comparable performance, the simpler model is preferred. This principle, known as Occam's Razor, suggests that simpler explanations are more likely to be correct than complex ones.
    -   **Advantages of Simplicity:** Simpler models are easier to understand, explain, and maintain. They are less likely to overfit and often require less data to train effectively.

In sum, understanding the key metrics that evaluate regression models provides insights into model performance, highlighting areas of strength and opportunities for improvement. A good model balances accuracy, generalizability, and simplicity, ensuring it can reliably predict outcomes and offer valuable insights into the phenomenon being studied.

### Interpreting the most optimal model results

Given that recipe3 lasso regression model gives the best prediction results both in terms of metrics and visualizations, we can conclude that the lasso model is the best and optimal model in this case. So, we will proceed to interpret its results first. However, we will also interpret  the recipe2 linear regression results because we didn't need to make any transformations to our variables and it is important to practice how to interpret them in this case.

## What to Look At in the Model Output:

1. **Coefficient (Estimate)**: Indicates the direction and magnitude of the relationship between each predictor and the dependent variable. Positive coefficients suggest a direct relationship, while negative coefficients suggest an inverse relationship. Positive signs indicate that as the predictor increases, the dependent variable tends to increase. The magnitude tells you how much change to expect in the dependent variable for a one-unit change in the predictor.

2. **Standard Error (Std. Error) [available only in lm regression]**: Measures the variability or uncertainty in the coefficient estimate. Lower values indicate more precise estimates.

3. **Statistic [available only in lm regression]**: This is typically the t-statistic, which tests the null hypothesis that the coefficient is equal to zero (no effect). It's calculated as the coefficient divided by its standard error. A higher absolute value of the t-statistic indeed indicates stronger evidence against the null hypothesis, which suggests that the coefficient is significantly different from zero. This means that the predictor variable is likely having a significant effect on the response variable.

4. **P-value**: Assesses the evidence against the null hypothesis. Low p-values (< 0.05) indicate strong evidence against the null hypothesis, suggesting the predictor has a statistically significant relationship with the dependent variable. P-value > 0.05 indicate that the relationship is not statistically significant.


### Step 11: Lasso recipe3 model interpretation
```{r}
recipe3_workflow_lasso |> tidy() |> print(n=33)#check model results

```

### Interpretation of Individual Predictors:
Variables that Lasso reduced to zero coefficients are considered not to have a statistically significant linear relationship with ADR after accounting for other variables and the regularization penalty.
Variables with non-zero coefficients are influencing factors for ADR and should be considered in pricing strategies. The negative coefficients for occupancy rate and cancellation rate are particularly noteworthy as they suggest areas where management could focus to improve revenue—possibly by managing cancellations better and optimizing occupancy rates to maximize revenue rather than just filling rooms.

#### Zero Coefficients Variables:
These are variables that Lasso has effectively removed from the model, indicating they do not provide additional predictive power given other variables in the model. So, we don't need to interpret the following:
- num_rooms
- distance to city center
- month
- number_of_bookings
- average_length_of_stay
- transport_accessibility_score
- city-specific dummies
- hotel chain specifics (except for InterContinental which has a very small positive impact compared to the baseline chain)
- seasons dummies

#### Interpretation of Significant Variables:

- *Intercept* (83.4): The intercept of $83.4 represents the expected ADR when all other predictor variables are at their baseline or reference levels. This means when all numerical predictors are zero and categorical predictors are in their baseline categories (in tidymodels the first one in alphabetical order). Given that zero might not be realistic for some variables (e.g., number of rooms or stars_rating), it's more useful to consider the intercept as the outcome value when predictors are at their lowest practical or observed levels (e.g., the minimum number of rooms that a hotel might have or the lowest star rating available in your data) within the dataset.

- *Star Rating* (0.171): For each additional star, the ADR increases by approximately 0.171 dollars, assuming all other variables remain constant. Given that the ADR is measured in dollars, this means an increase from a 1-star to a 5-star hotel, representing an increment of 4 stars, results in an increase of 0.171 x 4 = 0.684 dollars in the ADR. This unexpectedly small increment is surprising and suggests that the pricing strategy may not be fully optimized, or that star ratings have a lesser impact on pricing than typically expected in this dataset.

- *Review Score* (0.106): Each additional point in the review score (on a scale of 1 to 5) increases the ADR by 0.106 dollars given all other variables staying constant. This shows the value customers place on higher-rated hotels, which can command higher prices. However, also in this case the impact is surprisingly small given that the review score can at most move from 1 to 5 stars.  

- *Occupancy Rate* (-1.14): For each percentage point increase in occupancy rate, the ADR decreases by $1.14. This negative relationship is surprising and somewhat counterintuitive, as one might expect higher occupancy rates to correlate with higher rates, particularly during periods of high demand. This result could suggest that higher occupancy is being driven by lower rates, potentially indicating that pricing strategies are not optimized to capitalize on increased demand. It's possible that in an effort to maintain high occupancy levels, hotels might be underpricing their rooms, thus not maximizing potential revenue."

- *Business Facilities* (1.71) and *Leisure Facilities* (2.76): Hotels with business and leisure facilities can charge an additional 1.71 dollars and 2.76 dollars, respectively, on average per night. Business facilities cater to guests traveling for work, who may require meeting spaces, business centers, or enhanced in-room connectivity. Leisure facilities, such as pools, spas, and fitness centers, enhance the appeal for guests seeking relaxation or recreational activities during their stay. The presence of these facilities not only justifies higher room rates but also strategically positions hotels to attract a more diverse and potentially higher-paying clientele. These amenities contribute to a differentiated hotel experience that can command premium pricing, reflecting their importance in customer choice and satisfaction. However, those amenities impact does not seem substantial at first glance, particularly when considering the potential costs associated with maintaining them. It's important to delve deeper into what these figures might imply (e.g., we might want to also test the interaction between business and leisure facilities in your regression model). 

- *RevPAR* (1.24): RevPAR, or Revenue per Available Room, is a critical performance metric in the hotel industry that combines the effects of room rates and occupancy levels. The regression analysis indicates that for each dollar increase in RevPAR, the ADR increases by $1.24. This positive correlation underscores a direct linkage between the hotel's revenue efficiency and its pricing power. Essentially, as hotels manage to either increase their occupancy rates while maintaining room rates, or raise room rates without losing occupancy, their overall revenue per available room enhances, which in turn justifies and supports higher average daily rates. This relationship highlights the effectiveness of a hotel's pricing strategy and its capacity to maximize revenue through optimal pricing and occupancy management.

- *Competitors Average Price* (0.0525): The analysis indicates that for each dollar increase in competitors' average prices, a hotel’s average daily rate (ADR) increases by only $0.0525. This modest increase suggests that while hotels are responsive to competitors' pricing changes, they are deliberately choosing not to match or exceed these price increases fully. This strategy could reflect a positioning approach where hotels aim to offer slightly lower prices to attract price-sensitive customers or to increase market share by appealing as a more economical option within their competitive set.

- *Customer Satisfaction Index* (-0.00498): Surprisingly, higher customer satisfaction slightly decreases ADR, possibly indicating that highly rated hotels might not be fully leveraging their reputation in pricing.  The coefficient for the Customer Satisfaction Index (CSI) is -0.00498. This implies that for every one-unit increase in the customer satisfaction index, the ADR decreases by approximately $0.00498, assuming other factors remain constant. Given the mean customer satisfaction index is 85.3, this relationship might seem counter intuitive because we typically expect higher customer satisfaction to correlate with higher pricing power and thus higher ADR. However, the negative coefficient suggests a potential under utilization of customer satisfaction in pricing strategy or that other factors in the model are dominating the pricing strategy to such an extent that increases in customer satisfaction do not translate into higher rates.

- *Cancellation Rate* (-0.570): For each percentage point increase in cancellation rates, the Average Daily Rate (ADR) decreases by $0.570. This relationship likely reflects the negative impact of booking cancellations on revenue, leading to lower ADRs as hotels might reduce prices in response to increased cancellation rates to attract more bookings and reduce the likelihood of rooms remaining unoccupied. While this drop in price can mitigate the risk of empty rooms, it also suggests potential revenue losses or instability in booking patterns, which might force hotels to adopt more aggressive pricing strategies.

- *Location Quality*: Being in an 'Excellent' location increases the ADR by 1.89 dollars, while being in a 'Good' location increases it by 0.952 dollars, compared to 'Average' (our baseline). Being in a 'Poor' location decreases it by 0.755 dollars compared to 'Average'. This gradient clearly shows how location quality impacts pricing. However, also in this case hotels are probably underselling their location advantages and how much customers are really willing to pay to stay at a better location. 


#### Extra: Regression recipe2 model interpretation
In real life you don't need to interpret also this model as our lasso recipe 3 was clearly the best and optimal model for our predictions. However, for illustrative purpose I will offer a brief interpretation also for the best lm model.
```{r}
recipe2_workflow_reg|> tidy()#check model results

```

### Regression (lm recipe2) Output Interpretation:
- *Intercept* (53.4): Highly significant with a statistic of 49.7 and a p-value near zero (<0.05). Small standard error as well (1.07). $53.4 is the expected ADR when all predictors are at their baseline or minimal practical levels. It represents the baseline pricing from which adjustments are made based on other factors. 

- *revPAR* (0.258): Highly significant with a statistic of 26.1 and a p-value < 0.001. Standard error is very small (0.00987). For each dollar increase in revenue per available room (revPAR), the ADR increases by $0.258. This underscores the direct impact of operational efficiency and room occupancy on pricing power. It suggests that enhancing room occupancy and overall revenue per room is critical for increasing ADR.

- *Competitors Average Price*  (0.233): Highly significant with a statistic of 26.3 and a p-value < 0.001. Standard error is very small (0.00885). For each dollar increase in the average prices of competitors leads to a $0.233 increase in a hotel’s ADR. This responsiveness to competitive pricing highlights the market-driven pricing strategy, indicating that hotels are keenly aware of their market position relative to competitors.

- *Star Rating* (0.939): Significant with a statistic of 5.14 and a p-value near zero (<0.05). Small standard error as well (0.183). For each additional star rating ADR will increase by approximately $0.939 given all other variables staying constant. This confirms the value guests place on higher-rated accommodations but is it increase reflect the true star difference? Probably not, there should be a bigger difference as pointed out before.

- *Location Quality*: Compared to the Average location quality all the location quality values are statistically significant (p-value <0.05). Standard Error and Statistic also offer positive indication of the location quality dummy variables. The only borderline but still significant is Poor against the Average location quality. Being in an Excellent location increases the ADR by 5.89 dollars compared to the baseline location (Average). This is a significant effect, emphasizing the premium that Excellent locations command. Good locations increase the ADR by 2.93 dollars compared to the baseline, also reflecting a significant premium though less than excellent locations. Finally, Poor locations decrease the ADR by $1.50 compared to the baseline. Showing than less than desirable location negatively impact the hotels' ADR.

- *Seasons*: Comparing Spring, Summer and Winter against the baseline Fall results in p-values that are not statistically significant ( all > 0.05). We should not worry to further interpret them. 


### Step 12: Making forecast by leveraging the best/optimal model

 Once you have assessed model prediction performance and interpreted the results, the final step is to use it to make predictions on data that do not contain your dependent variable value (e.g., imagine that the hotel chains wants you to forecast future ADR for their rooms).
However, before we do that it is probably a good idea to save your best/optimal tidymodels pipeline. In the context of using the `tidymodels` framework for your modeling process, your `final_model` should be the entire workflow that you've created, fitted, and evaluated. This workflow includes both the preprocessing steps (defined in your recipe) and the model specification, along with its fitted parameters on the training data. Here's a breakdown of what's being saved and why:

#### What to Save as `final_model`:

- **The Fitted Workflow (`recipe3_workflow_lasso`)**: This is the object you've created that encapsulates:
  - The preprocessing recipe (`recipe3`), which defines how your data should be processed before modeling. This might include scaling, centering, encoding categorical variables, imputing missing values, etc.
  - The model specification (`linear_mod_lasso`), which defines the type of model you're using (in this case, a lasso regression model) and its configuration.
  - The fitted model parameters, which are the result of training (fitting) this model on your training dataset (`hotel_train`).

### Why Save the Entire Workflow:

- **Consistency in Preprocessing**: Saving the workflow pipeline ensures that any future data you wish to predict on undergoes the exact same preprocessing steps as your training data. This is crucial for the reliability of your predictions.
- **Ease of Use**: When you want to make predictions on new data, having the preprocessing and model encapsulated in a single object means you only need to deal with one object, rather than separately managing preprocessing steps and the model.
- **Reproducibility**: Saving the fitted workflow as your `final_model` ensures that you can reproduce your modeling process exactly, from preprocessing through to prediction. This is vital for verifying your results and for any future analysis.

### Saving the Workflow:

To save your `final_model` (in this case, the fitted workflow `recipe1_workflow_reg`), you would typically use the `saveRDS()` function in R:

```{r}
saveRDS(recipe3_workflow_lasso, "final_hotel_model.rds")
```

And when you're ready to use this model in the future, you can load it with `readRDS()` and immediately start making predictions on new data:

```{r}
final_model <- readRDS("final_hotel_model.rds")# to load it and use it in future sessions.
new_hotel_data <-  readr::read_csv("new_hotel_data.csv")# importing a dataset of houses without price in R.
new_predictions <- predict(final_model, new_data = new_hotel_data)# making the predictions on these new houses 
```

This process streamlines the application of your model to new datasets and ensures that all aspects of your model—preprocessing through prediction—are consistent and reproducible. Here's a simplified overview of the process:

1. **Preparing `new_hotel_data`**: When you collect or prepare new data for prediction, it should include all the features used during the training of your model but exclude the target variable (`average_daily_rate` in the hotel dataset context). This ensures that your model, which has learned the relationship between the features and the target variable during training, can now apply this knowledge to estimate the target for new, unseen data.

2. **Applying the Workflow**: Your saved workflow, when applied to `new_hotel_data`, performs the necessary preprocessing steps defined in your `recipe` (e.g., any manipulation or encoding categorical variables) on the new data. Since these steps were calibrated on your training data, they prepare `new_hotel_data` in a way that's consistent with the training process.

3. **Making Predictions**: With the preprocessed features, your model then predicts the target variable for each observation in `new_hotel_data`. These predictions are the model's estimates of what the average_daily_rates would be, based on the relationships it learned during training.

4. **Using Predictions**: The output you get can be used in various ways depending on your project's goals—whether it's setting room ADR, identifying under- or overvalued rooms, or further analysis and insights generation.

We have now completed a full modeling analysis and applied the model to forecast future average_daily_rates. Mastering modeling skills takes time and a lot of trials and error. That is why you should never use the first model build to make predictions about the future. Nonetheless, now you have the tools to evaluate different models and to interpret their results. Those will prove useful when you move into the professional world.



### Appendix A: Interpreting Ridge Results

The interpretation of results from Ridge regression analyses involves understanding the impact of regularization on the coefficients and how these coefficients describe the relationship between predictors and the response variable.

Ridge regression applies an L2 penalty to the coefficients, shrinking them towards zero but not setting any to exactly zero. The key aspects to interpret in Ridge regression results are:

- Coefficient Size: The magnitude of the coefficients indicates the relative importance of each predictor in relation to the average_daily_rate. However, due to the penalty applied, these values are generally smaller than those you might find in an ordinary least squares (OLS) regression. This shrinkage helps mitigate the risk of overfitting, particularly in situations with many correlated predictors.

- Significance: Unlike Lasso, Ridge does not perform feature selection. All predictors remain in the model but with reduced coefficients. The t-statistics and associated p-values can still help determine the significance of predictors, but the primary focus is often on the magnitude and direction of the coefficients. In fact, glm models will not produce t-statistic and pvalues. So, what to do? Use the magnitude to determine significance. The absolute size of the coefficient can give a general idea of the variable's importance: A larger absolute value of the coefficient generally suggests that the variable has a more significant impact on the response variable than a smaller absolute value.
